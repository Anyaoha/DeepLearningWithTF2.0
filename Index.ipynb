{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Index.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhiraiyan/DeepLearningWithTF2.0/blob/master/Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8wN5BpUKYFHA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with Tensorflow 2.0"
      ]
    },
    {
      "metadata": {
        "id": "fwY_CgcMYFAr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src='https://raw.githubusercontent.com/mukeshmithrakumar/DeepLearningWithTF2.0/master/logo/Deeplearningwithtensorflow20c.png' , width=225/>\n",
        "\n",
        "  <p>A practical guide to Ian Goodfellow's <a href=\"https://www.deeplearningbook.org/\">Deep Learning</a> book with Tensorflow 2.0 by Mukesh Mithrakumar. \n",
        "  The content is available <a href=\"https://github.com/adhiraiyan/DeepLearningWithTF2.0\">on GitHub</a> and you can run it in <a\n",
        "   href=\"https://colab.research.google.com/github.com/adhiraiyan/DeepLearningWithTF2.0/blob/master/notebooks/00.00-Preface.ipynb\">\n",
        "  Google Colaboratory</a> as well. The code is released under the <a href=\"https://opensource.org/licenses/MIT\">MIT license</a>.<br>\n",
        " </p>\n",
        "\n",
        "If you find this content useful, consider buying me a cup of coffee ‚òïÔ∏èüòâ.\n",
        "\n",
        "<div style=\"padding-top:10px;\">\n",
        "<span class=\"badge-buymeacoffee\" style=\"padding-right:5px;\"><a href=\"https://buymeacoffee.com/mmukesh\"\n",
        "        title=\"Donate to this project using Buy Me A Coffee\"><img\n",
        "            src=\"https://img.shields.io/badge/buy%20me%20a%20coffee-donate-blue.svg\"\n",
        "            alt=\"Buy Me A Coffee donate button\" /></a></span>\n",
        "<span class=\"badge-paypal\" style=\"padding-right:5px;\"><a href=\"https://paypal.me/mukeshmithrakumar\"\n",
        "        title=\"Donate to this project using Paypal\"><img\n",
        "            src=\"https://img.shields.io/badge/paypal-donate-green.svg\"\n",
        "            alt=\"PayPal donate button\" /></a></span>\n",
        "<span class=\"badge-patreon\"><a href=\"https://patreon.com/mukeshmithrakumar\"\n",
        "        title=\"Donate to this project using Patreon\"><img\n",
        "            src=\"https://img.shields.io/badge/patreon-donate-orange.svg\"\n",
        "            alt=\"Patreon donate button\" /></a></span>\n",
        "<span class=\"liberpay badge\" style=\"padding-right:5px;\"><a\n",
        "        href=\"https://liberapay.com/mmukesh/donate\"><img alt=\"Donate using Liberapay\"\n",
        "            src=\"https://liberapay.com/assets/widgets/donate.svg\"></a></span>\n",
        "</div>\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "w0FfjM53YE4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Table of Contents</h2>\n",
        "\n",
        "  <a href=\"00.00-Preface.ipynb\">\n",
        "    <h2>0. Preface</h2>\n",
        "  </a>\n",
        "\n",
        "  <a href=\"01.00-Introduction.ipynb\">\n",
        "    <h2>1. Introduction</h2>\n",
        "  </a>\n",
        "  <li>01.01 Who should read this book</li>\n",
        "  <li>01.02 Historical Trends in Deep Learning</li>\n",
        "\n",
        "  <a href=\"02.00-Linear-Algebra.ipynb\">\n",
        "    <h2>2. Applied Math and Machine Learning Basics</h2>\n",
        "  </a>\n",
        "  <li>02.01 Scalars, Vectors, Matrices and Tensors</li>\n",
        "  <li>02.02 Multiplying Matrices and Vectors</li>\n",
        "  <li>02.03 Identity and Inverse Matrices</li>\n",
        "  <li>02.04 Linear Dependence and Span</li>\n",
        "  <li>02.05 Norms</li>\n",
        "  <li>02.06 Special Kinds of Matrices and Vectors</li>\n",
        "  <li>02.07 Eigendecomposition</li>\n",
        "  <li>02.08 Singular Value Decomposition</li>\n",
        "  <li>02.09 The Moore-Penrose Pseudoinverse</li>\n",
        "  <li>02.10 The Trace Operator</li>\n",
        "  <li>02.11 The Determinant</li>\n",
        "  <li>02.12 Example: Principal Components Analysis</li>\n",
        "\n",
        "\n",
        "  <a href=\"03.00-Probability-and-Information-Theory.ipynb\">\n",
        "    <h2>3. Probability and Information Theory</h2>\n",
        "  </a>\n",
        "  <li>03.01 Why Probability?</li>\n",
        "  <li>03.02 Random Variables</li>\n",
        "  <li>03.03 Probability Distributions</li>\n",
        "  <li>03.04 Marginal Probability</li>\n",
        "  <li>03.05 Conditional Probability</li>\n",
        "  <li>03.06 The Chain Rule of Conditional Probabilities</li>\n",
        "  <li>03.07 Independence and Conditional Independence</li>\n",
        "  <li>03.08 Expectation, Variance and Covariance</li>\n",
        "  <li>03.09 Common Probability Distributions</li>\n",
        "  <li>03.10 Useful Properties of Common Functions</li>\n",
        "  <li>03.11 Bayes' Rule</li>\n",
        "  <li>03.12 Technical Details of Continuous Variables</li>\n",
        "  <li>03.13 Information Theory</li>\n",
        "  <li>03.14 Structured Probabilistic Models</li>\n",
        "\n",
        "\n",
        "  <a href=\"04.00-Numerical-Computation.ipynb\">\n",
        "    <h2>4. Numerical Computation</h2>\n",
        "  </a>\n",
        "  <li>04.01 Overflow and Underflow</li>\n",
        "  <li>04.02 Poor Conditioning</li>\n",
        "  <li>04.03 Gradient-Based Optimization</li>\n",
        "  <li>04.04 Constrained Optimization</li>\n",
        "  <li>04.05 Example: Linear Least Squares</li>\n",
        "\n",
        "\n",
        "  <a href=\"05.00-Machine-Learning-Basics.ipynb\">\n",
        "    <h2>5. Machine Learning Basics</h2>\n",
        "  </a>\n",
        "  <li>05.01 Learning Algorithms</li>\n",
        "  <li>05.02 Capacity, Overfitting and Underfitting</li>\n",
        "  <li>05.03 Hyperparameters and Validation Sets</li>\n",
        "  <li>05.04 Estimators, Bias and Variance</li>\n",
        "  <li>05.05 Maximum Likelihood Estimation</li>\n",
        "  <li>05.06 Bayesian Statistics</li>\n",
        "  <li>05.07 Supervised Learning Algorithms</li>\n",
        "  <li>05.08 Unsupervised Learning Algorithms</li>\n",
        "  <li>05.09 Stochastic Gradient Descent</li>\n",
        "  <li>05.10 Building a Machine Learning Algorithm</li>\n",
        "  <li>05.11 Challenges Motivating Deep Learning</li>\n",
        "\n",
        "\n",
        "  <a href=\"06.00-Deep-Feedforward-Networks.ipynb\">\n",
        "    <h2>6. Deep Feedforward Networks</h2>\n",
        "  </a>\n",
        "  <li>06.01 Example: Learning XOR</li>\n",
        "  <li>06.02 Gradient-Based Learning</li>\n",
        "  <li>06.03 Hidden Units</li>\n",
        "  <li>06.04 Architecture Design</li>\n",
        "  <li>06.05 Back-Propagation and Other Differentiation Algorithms</li>\n",
        "  <li>06.06 Historical Notes</li>\n",
        "\n",
        "\n",
        "  <a href=\"07.00-Regularization-for-Deep-Learning.ipynb\">\n",
        "    <h2>7. Regularization for Deep Learning</h2>\n",
        "  </a>\n",
        "  <li>07.01 Parameter Norm Penalties</li>\n",
        "  <li>07.02 Norm Penalties as Constrained Optimization</li>\n",
        "  <li>07.03 Regularization and Under-Constrained Problems</li>\n",
        "  <li>07.04 Dataset Augmentation</li>\n",
        "  <li>07.05 Noise Robustness</li>\n",
        "  <li>07.06 Semi-Supervised Learning</li>\n",
        "  <li>07.07 Multitask Learning</li>\n",
        "  <li>07.08 Early Stopping</li>\n",
        "  <li>07.09 Parameter Tying and Parameter Sharing</li>\n",
        "  <li>07.10 Sparse Representations</li>\n",
        "  <li>07.11 Bagging and Other Ensemble Methods</li>\n",
        "  <li>07.12 Dropout</li>\n",
        "  <li>07.13 Adversarial Training</li>\n",
        "  <li>07.14 Tangent Distance, Tangent Prop and Manifold Tangent Classifier</li>\n",
        "\n",
        "\n",
        "  <a href=\"08.00-Optimization-for-Training-Deep-Models.ipynb\">\n",
        "    <h2>8. Optimization for Training Deep Models</h2>\n",
        "  </a>\n",
        "  <li>08.01 How Learning Differs from Pure Optimization</li>\n",
        "  <li>08.02 Challenges in Neural Network Optimization</li>\n",
        "  <li>08.03 Basic Algorithms</li>\n",
        "  <li>08.04 Parameter Initialization Strategies</li>\n",
        "  <li>08.05 Algorithms with Adaptive Learning Rates</li>\n",
        "  <li>08.06 Approximate Second-Order Methods</li>\n",
        "  <li>08.07 Optimization Strategies and Meta-Algorithms</li>\n",
        "\n",
        "\n",
        "  <a href=\"09.00-Convolutional-Networks.ipynb\">\n",
        "    <h2>9. Convolutional Networks</h2>\n",
        "  </a>\n",
        "  <li>09.01 The Convolution Operation</li>\n",
        "  <li>09.02 Motivation</li>\n",
        "  <li>09.03 Pooling</li>\n",
        "  <li>09.04 Convolution and Pooling as an Infinitely Strong Prior</li>\n",
        "  <li>09.05 Variants of the Basic Convolution Function</li>\n",
        "  <li>09.06 Structured Outputs</li>\n",
        "  <li>09.07 Data Types</li>\n",
        "  <li>09.08 Efficient Convolution Algorithms</li>\n",
        "  <li>09.09 Random or Unsupervised Features</li>\n",
        "  <li>09.10 The Neuroscientific Basis for Convolutional Networks</li>\n",
        "  <li>09.11 Convolutional Networks and the History of Deep Learning</li>\n",
        "\n",
        "\n",
        "  <a href=\"10.00-Sequence-Modeling-Recurrent-and-Recursive-Nets.ipynb\">\n",
        "    <h2>10. Sequence Modeling: Recurrent and Recursive Nets</h2>\n",
        "  </a>\n",
        "  <li>10.01 Unfolding Computational Graphs</li>\n",
        "  <li>10.02 Recurrent Neural Networks</li>\n",
        "  <li>10.03 Bidirectional RNNs</li>\n",
        "  <li>10.04 Encoder-Decoder Sequence-to-Sequence Architectures</li>\n",
        "  <li>10.05 Deep Recurrent Networks</li>\n",
        "  <li>10.06 Recursive Neural Networks</li>\n",
        "  <li>10.07 The Challenge of Long-Term Dependencies</li>\n",
        "  <li>10.08 Echo State Networks</li>\n",
        "  <li>10.09 Leaky Units and Other Strategies for Multiple Time Scales</li>\n",
        "  <li>10.10 The Long Short-Term Memory and Other Gated RNNs</li>\n",
        "  <li>10.11 Optimization for Long-Term Dependencies</li>\n",
        "  <li>10.12 Explicit Memory</li>\n",
        "\n",
        "\n",
        "  <a href=\"11.00-Practical-Methodology.ipynb\">\n",
        "    <h2>11. Practical Methodology</h2>\n",
        "  </a>\n",
        "  <li>11.01 Performance Metrics</li>\n",
        "  <li>11.02 Default Baseline Models</li>\n",
        "  <li>11.03 Determining Whether to Gather More Data</li>\n",
        "  <li>11.04 Selecting Hyperparameters</li>\n",
        "  <li>11.05 Debugging Strategies</li>\n",
        "  <li>11.06 Example: Multi-Digit Number Recognition</li>\n",
        "\n",
        "\n",
        "  <a href=\"12.00-Applications.ipynb\">\n",
        "    <h2>12. Applications</h2>\n",
        "  </a>\n",
        "  <li>12.01 Large-Scale Deep Learning</li>\n",
        "  <li>12.02 Computer Vision</li>\n",
        "  <li>12.03 Speech Recognition</li>\n",
        "  <li>12.04 Natural Language Processing</li>\n",
        "  <li>12.05 Other Applications</li>\n",
        "\n",
        "\n",
        "  <a href=\"13.00-Linear-Factor-Models.ipynb\">\n",
        "    <h2>13. Linear Factor Models</h2>\n",
        "  </a>\n",
        "  <li>13.01 Probabilistic PCA and Factor Analysis</li>\n",
        "  <li>13.02 Independent Component Analysis</li>\n",
        "  <li>13.03 Slow Feature Analysis</li>\n",
        "  <li>13.04 Sparse Coding</li>\n",
        "  <li>13.05 Manifold Interpretation of PCA</li>\n",
        "\n",
        "\n",
        "  <a href=\"14.00-Autoencoders.ipynb\">\n",
        "    <h2>14. Autoencoders</h2>\n",
        "  </a>\n",
        "  <li>14.01 Undercomplete Autoencoders</li>\n",
        "  <li>14.02 Regularized Autoencoders</li>\n",
        "  <li>14.03 Representational Power, Layer Size and Depth</li>\n",
        "  <li>14.04 Stochastic Encoders and Decoders</li>\n",
        "  <li>14.05 Denoising Autoencoders</li>\n",
        "  <li>14.06 Learning Manifolds with Autoencoders</li>\n",
        "  <li>14.07 Contractive Autoencoders</li>\n",
        "  <li>14.08 Predictive Sparse Decomposition</li>\n",
        "  <li>14.09 Applications of Autoencoders</li>\n",
        "\n",
        "\n",
        "  <a href=\"15.00-Representation-Learning.ipynb\">\n",
        "    <h2>15. Representation Learning</h2>\n",
        "  </a>\n",
        "  <li>15.01 Greedy Layer-Wise Unsupervised Pretraining</li>\n",
        "  <li>15.02 Transfer Learning and Domain Adaptation</li>\n",
        "  <li>15.03 Semi-Supervised Disentangling of Causal Factors</li>\n",
        "  <li>15.04 Distributed Representation</li>\n",
        "  <li>15.05 Exponential Gains from Depth</li>\n",
        "  <li>15.06 Providing Clues to Discover Underlying Causes</li>\n",
        "\n",
        "\n",
        "  <a href=\"16.00-Structured-Probabilistic-Models-for-Deep-Learning.ipynb\">\n",
        "    <h2>16. Structured Probabilistic Models for Deep Learning</h2>\n",
        "  </a>\n",
        "  <li>16.01 The Challenge of Unstructured Modeling</li>\n",
        "  <li>16.02 Using Graphs to Describe Model Structure</li>\n",
        "  <li>16.03 Sampling from Graphical Models</li>\n",
        "  <li>16.04 Advantages of Structured Modeling</li>\n",
        "  <li>16.05 Learning about Dependencies</li>\n",
        "  <li>16.06 Inference and Approximate Inference</li>\n",
        "  <li>16.07 The Deep Learning Approach to Structured Probabilistic Models</li>\n",
        "\n",
        "\n",
        "  <a href=\"17.00-Monte-Carlo-Methods.ipynb\">\n",
        "    <h2>17. Monte Carlo Methods</h2>\n",
        "  </a>\n",
        "  <li>17.01 Sampling and Monte Carlo Methods</li>\n",
        "  <li>17.02 Importance Sampling</li>\n",
        "  <li>17.03 Markov Chain Monte Carlo Methods</li>\n",
        "  <li>17.04 Gibbs Sampling</li>\n",
        "  <li>17.05 The Challenge of Mixing between Separated Modes</li>\n",
        "\n",
        "\n",
        "  <a href=\"18.00-Confronting-the-Partition-Function.ipynb\">\n",
        "    <h2>18. Confronting the Partition Function</h2>\n",
        "  </a>\n",
        "  <li>18.01 The Log-Likelihood Gradient</li>\n",
        "  <li>18.02 Stochastic Maximum Likelihood and Contrastive Divergence</li>\n",
        "  <li>18.03 Pseudolikelihood</li>\n",
        "  <li>18.04 Score Matching and Ratio Matching</li>\n",
        "  <li>18.05 Denoising Score Matching</li>\n",
        "  <li>18.06 Noise-Contrastive Estimation</li>\n",
        "  <li>18.07 Estimating the Partition Function</li>\n",
        "\n",
        "\n",
        "  <a href=\"19.00-Approximate-Inference.ipynb\">\n",
        "    <h2>19. Approximate Inference</h2>\n",
        "  </a>\n",
        "  <li>19.01 Inference as Optimization</li>\n",
        "  <li>19.02 Expectation Maximization</li>\n",
        "  <li>19.03 MAP Inference and Sparse Coding</li>\n",
        "  <li>19.04 Variational Inference and Learning</li>\n",
        "  <li>19.05 Learned Approximate Inference</li>\n",
        "\n",
        "\n",
        "  <a href=\"20.00-Deep-Generative-Models.ipynb\">\n",
        "    <h2>20. Deep Generative Models</h2>\n",
        "  </a>\n",
        "  <li>20.01 Boltzmann Machines</li>\n",
        "  <li>20.02 Restricted Boltzmann Machines</li>\n",
        "  <li>20.03 Deep Belief Networks</li>\n",
        "  <li>20.04 Deep Boltzmann Machines</li>\n",
        "  <li>20.05 Boltzmann Machines for Real-Valued Data</li>\n",
        "  <li>20.06 Convolutional Boltzmann Machines</li>\n",
        "  <li>20.07 Boltzmann Machines for Structured or Sequential Outputs</li>\n",
        "  <li>20.08 Other Boltzmann Machines</li>\n",
        "  <li>20.09 Back-Propagation through Random Operations</li>\n",
        "  <li>20.10 Directed Generative Nets</li>\n",
        "  <li>20.11 Drawing Samples from Autoencoders</li>\n",
        "  <li>20.12 Generative Stochastic Networks</li>\n",
        "  <li>20.13 Other Generation Schemes</li>\n",
        "  <li>20.14 Evaluating Generative Models</li>\n",
        "  <li>20.15 Conclusion</li>"
      ]
    }
  ]
}