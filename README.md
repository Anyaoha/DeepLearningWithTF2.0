<p align="center"><img width="40%" src="logo/Deeplearningwithtensorflow20c.png" /></p>

<h1 align="center">Deep Learning with Tensorflow 2.0</h1>

<p align="center">
    <a href="https://opensource.org/licenses/MIT">
        <img src="https://img.shields.io/cocoapods/l/AFNetworking.svg" alt="GitHub">
    </a>
    <a href="https://www.python.org/downloads/release/python-360/">
        <img src="https://img.shields.io/badge/Python-3.6-blue.svg" alt="Python 3.6">
    </a>
    <a href="https://www.tensorflow.org/alpha">
        <img src="https://img.shields.io/badge/Tensorflow-2.0-orange.svg" alt="Python 3.6">
    </a>
    <a><img src="https://img.shields.io/badge/Status-Work_In_Progress-yellow.svg" alt="WorkInProgress"></a>
    <a href="https://www.linkedin.com/in/mukesh-mithrakumar/">
        <img src="https://img.shields.io/badge/LinkedIn-blue.svg?" alt="LinkedIn">
    </a>
    <a href="https://www.facebook.com/adhiraiyan/">
        <img src="https://img.shields.io/badge/Facebook-brightgreen.svg?" alt="Facebook">
    </a>
    <a href="https://twitter.com/MMithrakumar">
        <img src="https://img.shields.io/badge/Twitter-9cf.svg?" alt="Facebook">
    </a>
    <a href="https://www.adhiraiyan.org/">
        <img src="https://img.shields.io/badge/Adhiraiyan AI Blog-red.svg?" alt="Facebook">
    </a>
</p>

<p align="center">
    <a href="#clipboard-getting-started">Getting Started</a> ‚Ä¢
    <a href="#about">About</a> ‚Ä¢
    <a href="#table-of-contents">Table of Contents</a> ‚Ä¢
    <a href="#donate">Donate</a> ‚Ä¢
    <a href="#acknowledgment">Acknowledgment</a> ‚Ä¢
    <a href="#speech_balloon-faq">FAQ</a> ‚Ä¢
</p>

<h6 align="center">Made by Mukesh Mithrakumar ‚Ä¢ :milky_way: <a href="https://mukeshmithrakumar.com">https://mukeshmithrakumar.com</a></h6>


This is the GitHub version of the Deep Learning with Tensorflow 2.0 by Mukesh Mithrakumar. Feel free to fork and watch for updates. The upcoming release dates are next to the Chapters.

If you find this content useful, consider buying me a cup of coffee ‚òïÔ∏èüòâ.

<span class="badge-buymeacoffee"><a href="https://buymeacoffee.com/mmukesh"
        title="Donate to this project using Buy Me A Coffee"><img
            src="https://img.shields.io/badge/buy%20me%20a%20coffee-donate-blue.svg"
            alt="Buy Me A Coffee donate button" /></a></span>


<h2 align="center">:clipboard: Getting Started</h2>

- Read the book in its entirety online at https://www.adhiraiyan.org/books.html

- Run the code using the Jupyter notebooks available in this repository's [notebooks](notebooks) directory.

- Launch executable versions of these notebooks using [Google Colab](http://colab.research.google.com): [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/mukeshmithrakumar/DeepLearningWithTF2.0/blob/master/notebooks/00.00-Preface.ipynb)

- (Will Come Live after the Book is finished)Launch a live notebook server with these notebooks using [binder](https://beta.mybinder.org/): [![Binder](https://mybinder.org/badge.svg)]()


<h2 align="center">About</h2>



<h2 align="center">Table of Contents</h2>

## [0. Preface](00.00-Preface.ipynb)


## [1. Introduction (05.05.2019)](01.00-Introduction.ipynb)
<li>01.01 Who should read this book</li>
<li>01.02 Historical Trends in Deep Learning</li>


## [2. Applied Math and Machine Learning Basics (05.12.2019)](02.00-Linear-Algebra.ipynb)
<li>02.01 Scalars, Vectors, Matrices and Tensors</li>
<li>02.02 Multiplying Matrices and Vectors</li>
<li>02.03 Identity and Inverse Matrices</li>
<li>02.04 Linear Dependence and Span</li>
<li>02.05 Norms</li>
<li>02.06 Special Kinds of Matrices and Vectors</li>
<li>02.07 Eigendecomposition</li>
<li>02.08 Singular Value Decomposition</li>
<li>02.09 The Moore-Penrose Pseudoinverse</li>
<li>02.10 The Trace Operator</li>
<li>02.11 The Determinant</li>
<li>02.12 Example: Principal Components Analysis</li>


## [3. Probability and Information Theory (05.19.2019)](03.00-Probability-and-Information-Theory.ipynb)
<li>03.01 Why Probability?</li>
<li>03.02 Random Variables</li>
<li>03.03 Probability Distributions</li>
<li>03.04 Marginal Probability</li>
<li>03.05 Conditional Probability</li>
<li>03.06 The Chain Rule of Conditional Probabilities</li>
<li>03.07 Independence and Conditional Independence</li>
<li>03.08 Expectation, Variance and Covariance</li>
<li>03.09 Common Probability Distributions</li>
<li>03.10 Useful Properties of Common Functions</li>
<li>03.11 Bayes' Rule</li>
<li>03.12 Technical Details of Continuous Variables</li>
<li>03.13 Information Theory</li>
<li>03.14 Structured Probabilistic Models</li>


## [4. Numerical Computation (05.26.2019)](04.00-Numerical-Computation.ipynb)
<li>04.01 Overflow and Underflow</li>
<li>04.02 Poor Conditioning</li>
<li>04.03 Gradient-Based Optimization</li>
<li>04.04 Constrained Optimization</li>
<li>04.05 Example: Linear Least Squares</li>


## [5. Machine Learning Basics (06.02.2019)](05.00-Machine-Learning-Basics.ipynb)
<li>05.01 Learning Algorithms</li>
<li>05.02 Capacity, Overfitting and Underfitting</li>
<li>05.03 Hyperparameters and Validation Sets</li>
<li>05.04 Estimators, Bias and Variance</li>
<li>05.05 Maximum Likelihood Estimation</li>
<li>05.06 Bayesian Statistics</li>
<li>05.07 Supervised Learning Algorithms</li>
<li>05.08 Unsupervised Learning Algorithms</li>
<li>05.09 Stochastic Gradient Descent</li>
<li>05.10 Building a Machine Learning Algorithm</li>
<li>05.11 Challenges Motivating Deep Learning</li>


## [6. Deep Feedforward Networks (06.09.2019)](06.00-Deep-Feedforward-Networks.ipynb)
<li>06.01 Example: Learning XOR</li>
<li>06.02 Gradient-Based Learning</li>
<li>06.03 Hidden Units</li>
<li>06.04 Architecture Design</li>
<li>06.05 Back-Propagation and Other Differentiation Algorithms</li>
<li>06.06 Historical Notes</li>


## [7. Regularization for Deep Learning (06.16.2019)](07.00-Regularization-for-Deep-Learning.ipynb)
<li>07.01 Parameter Norm Penalties</li>
<li>07.02 Norm Penalties as Constrained Optimization</li>
<li>07.03 Regularization and Under-Constrained Problems</li>
<li>07.04 Dataset Augmentation</li>
<li>07.05 Noise Robustness</li>
<li>07.06 Semi-Supervised Learning</li>
<li>07.07 Multitask Learning</li>
<li>07.08 Early Stopping</li>
<li>07.09 Parameter Tying and Parameter Sharing</li>
<li>07.10 Sparse Representations</li>
<li>07.11 Bagging and Other Ensemble Methods</li>
<li>07.12 Dropout</li>
<li>07.13 Adversarial Training</li>
<li>07.14 Tangent Distance, Tangent Prop and Manifold Tangent Classifier</li>


## [8. Optimization for Training Deep Models (06.23.2019)](08.00-Optimization-for-Training-Deep-Models.ipynb)
<li>08.01 How Learning Differs from Pure Optimization</li>
<li>08.02 Challenges in Neural Network Optimization</li>
<li>08.03 Basic Algorithms</li>
<li>08.04 Parameter Initialization Strategies</li>
<li>08.05 Algorithms with Adaptive Learning Rates</li>
<li>08.06 Approximate Second-Order Methods</li>
<li>08.07 Optimization Strategies and Meta-Algorithms</li>


## [9. Convolutional Networks (06.30.2019)](09.00-Convolutional-Networks.ipynb)
<li>09.01 The Convolution Operation</li>
<li>09.02 Motivation</li>
<li>09.03 Pooling</li>
<li>09.04 Convolution and Pooling as an Infinitely Strong Prior</li>
<li>09.05 Variants of the Basic Convolution Function</li>
<li>09.06 Structured Outputs</li>
<li>09.07 Data Types</li>
<li>09.08 Efficient Convolution Algorithms</li>
<li>09.09 Random or Unsupervised Features</li>
<li>09.10 The Neuroscientific Basis for Convolutional Networks</li>
<li>09.11 Convolutional Networks and the History of Deep Learning</li>


## [10. Sequence Modeling: Recurrent and Recursive Nets (07.07.2019)](10.00-Sequence-Modeling-Recurrent-and-Recursive-Nets.ipynb)
<li>10.01 Unfolding Computational Graphs</li>
<li>10.02 Recurrent Neural Networks</li>
<li>10.03 Bidirectional RNNs</li>
<li>10.04 Encoder-Decoder Sequence-to-Sequence Architectures</li>
<li>10.05 Deep Recurrent Networks</li>
<li>10.06 Recursive Neural Networks</li>
<li>10.07 The Challenge of Long-Term Dependencies</li>
<li>10.08 Echo State Networks</li>
<li>10.09 Leaky Units and Other Strategies for Multiple Time Scales</li>
<li>10.10 The Long Short-Term Memory and Other Gated RNNs</li>
<li>10.11 Optimization for Long-Term Dependencies</li>
<li>10.12 Explicit Memory</li>


## [11. Practical Methodology (07.14.2019)](11.00-Practical-Methodology.ipynb)
<li>11.01 Performance Metrics</li>
<li>11.02 Default Baseline Models</li>
<li>11.03 Determining Whether to Gather More Data</li>
<li>11.04 Selecting Hyperparameters</li>
<li>11.05 Debugging Strategies</li>
<li>11.06 Example: Multi-Digit Number Recognition</li>


## [12. Applications (07.21.2019)](12.00-Applications.ipynb)
<li>12.01 Large-Scale Deep Learning</li>
<li>12.02 Computer Vision</li>
<li>12.03 Speech Recognition</li>
<li>12.04 Natural Language Processing</li>
<li>12.05 Other Applications</li>


## [13. Linear Factor Models (07.28.2019)](13.00-Linear-Factor-Models.ipynb)
<li>13.01 Probabilistic PCA and Factor Analysis</li>
<li>13.02 Independent Component Analysis</li>
<li>13.03 Slow Feature Analysis</li>
<li>13.04 Sparse Coding</li>
<li>13.05 Manifold Interpretation of PCA</li>


## [14. Autoencoders (08.04.2019)](14.00-Autoencoders.ipynb)
<li>14.01 Undercomplete Autoencoders</li>
<li>14.02 Regularized Autoencoders</li>
<li>14.03 Representational Power, Layer Size and Depth</li>
<li>14.04 Stochastic Encoders and Decoders</li>
<li>14.05 Denoising Autoencoders</li>
<li>14.06 Learning Manifolds with Autoencoders</li>
<li>14.07 Contractive Autoencoders</li>
<li>14.08 Predictive Sparse Decomposition</li>
<li>14.09 Applications of Autoencoders</li>


## [15. Representation Learning (08.11.2019)](15.00-Representation-Learning.ipynb)
<li>15.01 Greedy Layer-Wise Unsupervised Pretraining</li>
<li>15.02 Transfer Learning and Domain Adaptation</li>
<li>15.03 Semi-Supervised Disentangling of Causal Factors</li>
<li>15.04 Distributed Representation</li>
<li>15.05 Exponential Gains from Depth</li>
<li>15.06 Providing Clues to Discover Underlying Causes</li>


## [16. Structured Probabilistic Models for Deep Learning (08.18.2019)](16.00-Structured-Probabilistic-Models-for-Deep-Learning.ipynb)
<li>16.01 The Challenge of Unstructured Modeling</li>
<li>16.02 Using Graphs to Describe Model Structure</li>
<li>16.03 Sampling from Graphical Models</li>
<li>16.04 Advantages of Structured Modeling</li>
<li>16.05 Learning about Dependencies</li>
<li>16.06 Inference and Approximate Inference</li>
<li>16.07 The Deep Learning Approach to Structured Probabilistic Models</li>


## [17. Monte Carlo Methods (08.25.2019)](17.00-Monte-Carlo-Methods.ipynb)
<li>17.01 Sampling and Monte Carlo Methods</li>
<li>17.02 Importance Sampling</li>
<li>17.03 Markov Chain Monte Carlo Methods</li>
<li>17.04 Gibbs Sampling</li>
<li>17.05 The Challenge of Mixing between Separated Modes</li>


## [18. Confronting the Partition Function (09.01.2019)](18.00-Confronting-the-Partition-Function.ipynb)
<li>18.01 The Log-Likelihood Gradient</li>
<li>18.02 Stochastic Maximum Likelihood and Contrastive Divergence</li>
<li>18.03 Pseudolikelihood</li>
<li>18.04 Score Matching and Ratio Matching</li>
<li>18.05 Denoising Score Matching</li>
<li>18.06 Noise-Contrastive Estimation</li>
<li>18.07 Estimating the Partition Function</li>


## [19. Approximate Inference (09.08.2019)](19.00-Approximate-Inference.ipynb)
<li>19.01 Inference as Optimization</li>
<li>19.02 Expectation Maximization</li>
<li>19.03 MAP Inference and Sparse Coding</li>
<li>19.04 Variational Inference and Learning</li>
<li>19.05 Learned Approximate Inference</li>


## [20. Deep Generative Models (09.15.2019)](20.00-Deep-Generative-Models.ipynb)
<li>20.01 Boltzmann Machines</li>
<li>20.02 Restricted Boltzmann Machines</li>
<li>20.03 Deep Belief Networks</li>
<li>20.04 Deep Boltzmann Machines</li>
<li>20.05 Boltzmann Machines for Real-Valued Data</li>
<li>20.06 Convolutional Boltzmann Machines</li>
<li>20.07 Boltzmann Machines for Structured or Sequential Outputs</li>
<li>20.08 Other Boltzmann Machines</li>
<li>20.09 Back-Propagation through Random Operations</li>
<li>20.10 Directed Generative Nets</li>
<li>20.11 Drawing Samples from Autoencoders</li>
<li>20.12 Generative Stochastic Networks</li>
<li>20.13 Other Generation Schemes</li>
<li>20.14 Evaluating Generative Models</li>
<li>20.15 Conclusion</li>


<h2 align="center">Donate</h2>

<h3>Who Am I?</h3>

I'm a freelancer living in Chengdu, China. I mostly do front-end and Node.js stuffs and love them üíï

Currently I'm making lots of front-end components, tools, Node.js libraries, websites and apps to help people solve problems, if you enjoy my works please consider making a donation. My ultimate goal is to become a full-time open-source ninja.

3 years ago I started devoting my time to open-source for fun, but things have changed a lot over time. Now I have a day job and I can't spend that much time in open-source anymore. But here's a way to work around this, which is, becoming my patron to fulfill my dream of doing full-time open-source:


- Donate via Paypal <span class="badge-paypal"><a href="https://paypal.me/mukeshmithrakumar" title="Donate to this project using Paypal"><img src="https://img.shields.io/badge/paypal-donate-green.svg" alt="PayPal donate button" /></a></span>

- Become a Patreon <span class="badge-patreon"><a href="https://patreon.com/mukeshmithrakumar" title="Donate to this project using Patreon"><img src="https://img.shields.io/badge/patreon-donate-orange.svg" alt="Patreon donate button" /></a></span>

- Donate via flattr <span class="badge-flattr"><a href="https://flattr.com/profile/mukeshmithrakumar" title="Donate to this project using Flattr"><img src="https://img.shields.io/badge/flattr-donate-brightgreen.svg" alt="Flattr donate button" /></a></span>

- Donate via liberpay <a href="https://liberapay.com/mmukesh/donate"><img alt="Donate using Liberapay" src="https://liberapay.com/assets/widgets/donate.svg"></a>

- Donate via Buy me a Cofee <span class="badge-buymeacoffee"><a href="https://buymeacoffee.com/mmukesh" title="Donate to this project using Buy Me A Coffee"><img src="https://img.shields.io/badge/buy%20me%20a%20coffee-donate-brightgreen.svg" alt="Buy Me A Coffee donate button" /></a></span>


<h2 align="center">Acknowledgment</h2>



<h2 align="center">:speech_balloon: FAQ</h2>
